lang.coef = 10,
uncxlang = 5
)
#  2) Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = ntot, replace = TRUE)})
env.samples
# Determine which environmental variables interact
intrxnname <- names(model.parameters)[5] # interaction terms
intrxnname
names.temp <- gsub("x", "|", intrxnname) # remove text to align with colnames
names.temp
env.pairs <- sapply(1:length(names.temp), FUN = function(X){
grep(pattern = names.temp[X], x = colnames(env.samples))
})
env.pairs
# Add these interactions (product) to env.samples
env.interactions <- sapply(1:ncol(env.pairs), FUN = function(X){
apply(env.samples[, env.pairs[, X]], MARGIN = 1, FUN = prod)
})
env.interactions
env.samples2 <- cbind(env.samples, env.interactions)
# Create model matrix
mm <- model.matrix(~env.samples2)
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ], sd = 10)})
response
rti_testdata_intrxn <- cbind(data.frame(intention = response, provider = env.samples[,1], uncertainty = env.samples[,2],
language = env.samples[,3]))
write.csv(rti_testdata_intrxn, file="~/Desktop/rti_testdata_intrxn.csv", row.names = FALSE)
#  7) Let's do a quick lmer model to test the fake data
modtest <- lm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn) ## Quick look looks good!
modtest
model.parameters <- list(intercept = 0,
prov.coef = -1,
unc.coef = 3,
lang.coef = 3,
uncxlang = 1
)
#  1) Let's make the observations much higher than the actual data to build a good model.
ntot = 175 # numbers of obs per species.
sample_a <- list(prov.env = rbinom(ntot, 1, 0.5),
unc.env = rbinom(ntot, 1, 0.5),
lang.env = rbinom(ntot, 1, 0.5))
model.parameters <- list(intercept = 0,
prov.coef = -1,
unc.coef = 3,
lang.coef = 3,
uncxlang = 1
)
#  2) Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = ntot, replace = TRUE)})
# Determine which environmental variables interact
intrxnname <- names(model.parameters)[5] # interaction terms
names.temp <- gsub("x", "|", intrxnname) # remove text to align with colnames
env.pairs <- sapply(1:length(names.temp), FUN = function(X){
grep(pattern = names.temp[X], x = colnames(env.samples))
})
# Add these interactions (product) to env.samples
env.interactions <- sapply(1:ncol(env.pairs), FUN = function(X){
apply(env.samples[, env.pairs[, X]], MARGIN = 1, FUN = prod)
})
env.samples2 <- cbind(env.samples, env.interactions)
# Create model matrix
mm <- model.matrix(~env.samples2)
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ], sd = 10)})
rti_testdata_intrxn <- cbind(data.frame(intention = response, provider = env.samples[,1], uncertainty = env.samples[,2],
language = env.samples[,3]))
write.csv(rti_testdata_intrxn, file="~/Desktop/rti_testdata_intrxn.csv", row.names = FALSE)
#  7) Let's do a quick lmer model to test the fake data
modtest <- lm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn) ## Quick look looks good!
modtest
display(modtest)
library(arm)
display(modtest)
#  1) Let's make the observations much higher than the actual data to build a good model.
ntot = 1000 # numbers of obs per species.
sample_a <- list(prov.env = rbinom(ntot, 1, 0.5),
unc.env = rbinom(ntot, 1, 0.5),
lang.env = rbinom(ntot, 1, 0.5))
model.parameters <- list(intercept = 0,
prov.coef = -5,
unc.coef = 10,
lang.coef = 10,
uncxlang = 5
)
#  2) Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = ntot, replace = TRUE)})
# Determine which environmental variables interact
intrxnname <- names(model.parameters)[5] # interaction terms
names.temp <- gsub("x", "|", intrxnname) # remove text to align with colnames
env.pairs <- sapply(1:length(names.temp), FUN = function(X){
grep(pattern = names.temp[X], x = colnames(env.samples))
})
# Add these interactions (product) to env.samples
env.interactions <- sapply(1:ncol(env.pairs), FUN = function(X){
apply(env.samples[, env.pairs[, X]], MARGIN = 1, FUN = prod)
})
env.samples2 <- cbind(env.samples, env.interactions)
# Create model matrix
mm <- model.matrix(~env.samples2)
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ], sd = 10)})
rti_testdata_intrxn <- cbind(data.frame(intention = response, provider = env.samples[,1], uncertainty = env.samples[,2],
language = env.samples[,3]))
write.csv(rti_testdata_intrxn, file="~/Desktop/rti_testdata_intrxn.csv", row.names = FALSE)
#  7) Let's do a quick lmer model to test the fake data
modtest <- lm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn) ## Quick look looks good!
display(modtest)
#  Let's make the observations much higher than the actual data to build a good model.
ntot = 175 # numbers of obs per species.
sample_a <- list(prov.env = rbinom(ntot, 1, 0.5),
unc.env = rbinom(ntot, 1, 0.5),
lang.env = rbinom(ntot, 1, 0.5))
model.parameters <- list(intercept = 0,
prov.coef = -5,
unc.coef = 10,
lang.coef = 10,
uncxlang = 5
)
#  Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = ntot, replace = TRUE)})
# Determine which environmental variables interact
intrxnname <- names(model.parameters)[5] # interaction terms
names.temp <- gsub("x", "|", intrxnname) # remove text to align with colnames
env.pairs <- sapply(1:length(names.temp), FUN = function(X){
grep(pattern = names.temp[X], x = colnames(env.samples))
})
# Add these interactions (product) to env.samples
env.interactions <- sapply(1:ncol(env.pairs), FUN = function(X){
apply(env.samples[, env.pairs[, X]], MARGIN = 1, FUN = prod)
})
env.samples2 <- cbind(env.samples, env.interactions)
# Create model matrix
mm <- model.matrix(~env.samples2)
parameters.temp <- matrix(unlist(model.parameters), ncol = length(model.parameters), nrow = ntot, byrow = TRUE)
# Generate parameters
parameters.temp[, 1] <- sapply(1:nsp, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)})
parameters.temp[, 2] <- sapply(1:nsp, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)})
parameters.temp[, 3] <- sapply(1:nsp, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)})
parameters.temp[, 3] <- sapply(1:nsp, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)})
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 1)})
# Generate parameters
parameters.temp[, 1] <- sapply(1, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)})
parameters.temp[, 2] <- sapply(1, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)})
parameters.temp[, 3] <- sapply(1, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)})
parameters.temp[, 3] <- sapply(1, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)})
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 1)})
# Generate parameters
parameters.temp[, 1] <- rep(rnorm(n = 1, mean = model.parameters, sd = 0.5), ntot)
mean = model.parameters
model.parameters
# Generate parameters
parameters.temp[, 1] <- rep(rnorm(n = 1, mean = model.parameters[[1]], sd = 0.5), ntot)
parameters.temp[, 2] <- rep(rnorm(n = 1, mean = model.parameters[[2]], sd = 0.5), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[3]], sd = 0.5), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[4]], sd = 0.5), ntot)
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 1)})
rti_testdata_intrxn <- cbind(data.frame(intention = response, provider = env.samples[,1], uncertainty = env.samples[,2],
language = env.samples[,3]))
write.csv(rti_testdata_intrxn, file="~/Desktop/rti_testdata_intrxn.csv", row.names = FALSE)
#  Let's do a quick lmer model to test the fake data
modtest <- lm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn) ## Quick look looks good!
summary(modtest)
#  Let's make the observations much higher than the actual data to build a good model.
ntot = 175 # numbers of obs per species.
sample_a <- list(prov.env = rbinom(ntot, 1, 0.5),
unc.env = rbinom(ntot, 1, 0.5),
lang.env = rbinom(ntot, 1, 0.5))
model.parameters <- list(intercept = 0,
prov.coef = -5,
unc.coef = 10,
lang.coef = 10,
uncxlang = 5
)
#  Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = ntot, replace = TRUE)})
# Determine which environmental variables interact
intrxnname <- names(model.parameters)[5] # interaction terms
names.temp <- gsub("x", "|", intrxnname) # remove text to align with colnames
env.pairs <- sapply(1:length(names.temp), FUN = function(X){
grep(pattern = names.temp[X], x = colnames(env.samples))
})
# Add these interactions (product) to env.samples
env.interactions <- sapply(1:ncol(env.pairs), FUN = function(X){
apply(env.samples[, env.pairs[, X]], MARGIN = 1, FUN = prod)
})
env.samples2 <- cbind(env.samples, env.interactions)
# Create model matrix
mm <- model.matrix(~env.samples2)
parameters.temp <- matrix(unlist(model.parameters), ncol = length(model.parameters), nrow = ntot, byrow = TRUE)
# Generate parameters
parameters.temp[, 1] <- rep(rnorm(n = 1, mean = model.parameters[[1]], sd = 2), ntot)
parameters.temp[, 2] <- rep(rnorm(n = 1, mean = model.parameters[[2]], sd = 2), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[3]], sd = 2), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[4]], sd = 2), ntot)
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 5)})
rti_testdata_intrxn <- cbind(data.frame(intention = response, provider = env.samples[,1], uncertainty = env.samples[,2],
language = env.samples[,3]))
write.csv(rti_testdata_intrxn, file="~/Desktop/rti_testdata_intrxn.csv", row.names = FALSE)
#  Let's do a quick lmer model to test the fake data
modtest <- lm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn) ## Quick look looks good!
summary(modtest)
#  Let's make the observations much higher than the actual data to build a good model.
ntot = 175 # numbers of obs per species.
sample_a <- list(prov.env = rbinom(ntot, 1, 0.5),
unc.env = rbinom(ntot, 1, 0.5),
lang.env = rbinom(ntot, 1, 0.5))
model.parameters <- list(intercept = 0,
prov.coef = -5,
unc.coef = 10,
lang.coef = 10,
uncxlang = 5
)
#  Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = ntot, replace = TRUE)})
# Determine which environmental variables interact
intrxnname <- names(model.parameters)[5] # interaction terms
names.temp <- gsub("x", "|", intrxnname) # remove text to align with colnames
env.pairs <- sapply(1:length(names.temp), FUN = function(X){
grep(pattern = names.temp[X], x = colnames(env.samples))
})
# Add these interactions (product) to env.samples
env.interactions <- sapply(1:ncol(env.pairs), FUN = function(X){
apply(env.samples[, env.pairs[, X]], MARGIN = 1, FUN = prod)
})
env.samples2 <- cbind(env.samples, env.interactions)
# Create model matrix
mm <- model.matrix(~env.samples2)
parameters.temp <- matrix(unlist(model.parameters), ncol = length(model.parameters), nrow = ntot, byrow = TRUE)
# Generate parameters
parameters.temp[, 1] <- rep(rnorm(n = 1, mean = model.parameters[[1]], sd = 1), ntot)
parameters.temp[, 2] <- rep(rnorm(n = 1, mean = model.parameters[[2]], sd = 1), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[3]], sd = 1), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[4]], sd = 1), ntot)
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 2)})
rti_testdata_intrxn <- cbind(data.frame(intention = response, provider = env.samples[,1], uncertainty = env.samples[,2],
language = env.samples[,3]))
write.csv(rti_testdata_intrxn, file="~/Desktop/rti_testdata_intrxn.csv", row.names = FALSE)
#  Let's do a quick lmer model to test the fake data
modtest <- lm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn) ## Quick look looks good!
summary(modtest)
df(modtest)
#  Let's do a quick lmer model to test the fake data
modtest <- glm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn, family=binomial(link="logit")) ## Quick look looks good!
#  Let's make the observations much higher than the actual data to build a good model.
ntot = 175 # numbers of obs per species.
sample_a <- list(prov.env = rbinom(ntot, 1, 0.5),
unc.env = rbinom(ntot, 1, 0.5),
lang.env = rbinom(ntot, 1, 0.5))
model.parameters <- list(intercept = 0,
prov.coef = -0.05,
unc.coef = .1,
lang.coef = .10,
uncxlang = .05
)
#  Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = ntot, replace = TRUE)})
# Determine which environmental variables interact
intrxnname <- names(model.parameters)[5] # interaction terms
names.temp <- gsub("x", "|", intrxnname) # remove text to align with colnames
env.pairs <- sapply(1:length(names.temp), FUN = function(X){
grep(pattern = names.temp[X], x = colnames(env.samples))
})
# Add these interactions (product) to env.samples
env.interactions <- sapply(1:ncol(env.pairs), FUN = function(X){
apply(env.samples[, env.pairs[, X]], MARGIN = 1, FUN = prod)
})
env.samples2 <- cbind(env.samples, env.interactions)
# Create model matrix
mm <- model.matrix(~env.samples2)
parameters.temp <- matrix(unlist(model.parameters), ncol = length(model.parameters), nrow = ntot, byrow = TRUE)
# Generate parameters
parameters.temp[, 1] <- rep(rnorm(n = 1, mean = model.parameters[[1]], sd = 1), ntot)
parameters.temp[, 2] <- rep(rnorm(n = 1, mean = model.parameters[[2]], sd = 1), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[3]], sd = 1), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[4]], sd = 1), ntot)
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 2)})
rti_testdata_intrxn <- cbind(data.frame(intention = response, provider = env.samples[,1], uncertainty = env.samples[,2],
language = env.samples[,3]))
write.csv(rti_testdata_intrxn, file="~/Desktop/rti_testdata_intrxn.csv", row.names = FALSE)
#  Let's do a quick lmer model to test the fake data
modtest <- glm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn, family=binomial(link="logit")) ## Quick look looks good!
response
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rbeta(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 2)})
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
dbeta(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 2)})
modtest.anova <- anova(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn)
modtest.anova <- anova(lm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn))
summary(modtest.anova)
modtest.anova
#  Let's make the observations much higher than the actual data to build a good model.
ntot = 175 # numbers of obs per species.
sample_a <- list(prov.env = rbinom(ntot, 1, 0.5),
unc.env = rbinom(ntot, 1, 0.5),
lang.env = rbinom(ntot, 1, 0.5))
model.parameters <- list(intercept = 0,
prov.coef = -5,
unc.coef = 10,
lang.coef = 10,
uncxlang = 5
)
#  Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = ntot, replace = TRUE)})
# Determine which environmental variables interact
intrxnname <- names(model.parameters)[5] # interaction terms
names.temp <- gsub("x", "|", intrxnname) # remove text to align with colnames
env.pairs <- sapply(1:length(names.temp), FUN = function(X){
grep(pattern = names.temp[X], x = colnames(env.samples))
})
# Add these interactions (product) to env.samples
env.interactions <- sapply(1:ncol(env.pairs), FUN = function(X){
apply(env.samples[, env.pairs[, X]], MARGIN = 1, FUN = prod)
})
env.samples2 <- cbind(env.samples, env.interactions)
# Create model matrix
mm <- model.matrix(~env.samples2)
parameters.temp <- matrix(unlist(model.parameters), ncol = length(model.parameters), nrow = ntot, byrow = TRUE)
# Generate parameters
parameters.temp[, 1] <- rep(rnorm(n = 1, mean = model.parameters[[1]], sd = 1), ntot)
parameters.temp[, 2] <- rep(rnorm(n = 1, mean = model.parameters[[2]], sd = 1), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[3]], sd = 1), ntot)
parameters.temp[, 3] <- rep(rnorm(n = 1, mean = model.parameters[[4]], sd = 1), ntot)
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 2)})
rti_testdata_intrxn <- cbind(data.frame(intention = response, provider = env.samples[,1], uncertainty = env.samples[,2],
language = env.samples[,3]))
write.csv(rti_testdata_intrxn, file="~/Desktop/rti_testdata_intrxn.csv", row.names = FALSE)
modtest.anova <- anova(lm(intention ~ provider + uncertainty + language + uncertainty*language, data=rti_testdata_intrxn))
summary(modtest.anova)
modtest.anova
# housekeeping
rm(list=ls())
options(stringsAsFactors = FALSE)
options(mc.cores = parallel::detectCores())
library(RColorBrewer)
library(viridis)
library(ggplot2)
library(gridExtra)
library(rstan)
setwd("~/Documents/git/microclimates/analyses/")
source("source/sims_hypoth_sourcedata.R")
source("source/sims_hypoth_interxn_sourcedata.R")
realgdd <- read.csv("output/cleanmicro_gdd_2019.csv")
simsdat <- bbfunc("hobo", "ws", 0, 15, 300, 20, 10, 3, 0)
xtext <- seq(1, 2, by=1)
cols <-viridis_pal(option="viridis")(3)
bball <- simsdat[[1]]
clim <- simsdat[[2]]
ws <- ggplot(clim[(clim$method=="ws"),], aes(x=tmean)) + geom_histogram(aes(fill=site), alpha=0.3) + theme_classic() +
scale_fill_manual(name="Site", values=cols, labels=c("Arboretum", "Harvard Forest")) + ggtitle("Weather Station") +
coord_cartesian(xlim=c(-20, 40)) +
geom_vline(xintercept=mean(clim$tmean[(clim$method=="ws" & clim$site=="arb")]), col=cols[[1]], linetype="dashed") +
geom_vline(xintercept=mean(clim$tmean[(clim$method=="ws" & clim$site=="hf")]), col=cols[[2]], linetype="dashed") +
xlab("Mean Temperature (°C)") + ylab("") +
scale_y_continuous(expand = c(0, 0)) +
scale_x_continuous(breaks = seq(-20, 40, by=5)) +
theme(legend.position="none")
hobo <- ggplot(clim[(clim$method=="hobo"),], aes(x=tmean)) + geom_histogram(aes(fill=site), alpha=0.3) + theme_classic() +
scale_fill_manual(name="Site", values=cols, labels=c("Arboretum", "Harvard Forest")) + ggtitle("Hobo Logger") +
geom_vline(xintercept=mean(clim$tmean[(clim$method=="hobo" & clim$site=="arb")]), col=cols[[1]], linetype="dashed") +
geom_vline(xintercept=mean(clim$tmean[(clim$method=="hobo" & clim$site=="hf")]), col=cols[[2]], linetype="dashed") +
coord_cartesian(xlim=c(-20, 40)) +
xlab("Mean Temperature (°C)") + ylab("") +
scale_y_continuous(expand = c(0, 0)) +
scale_x_continuous(breaks = seq(-20, 40, by=5))
pdf("figures/clim_methods_noisyws.pdf", width=8, height=4, onefile=FALSE)
egg::ggarrange(ws, hobo, ncol=2)
dev.off()
ws <- ggplot(bball[(bball$method=="ws"),], aes(x=gdd)) + geom_histogram(aes(fill=site), alpha=0.3, position="stack") +
theme_classic() +
scale_fill_manual(name="Site", values=cols, labels=c("Arboretum", "Harvard Forest")) + ggtitle("Weather Station") +
coord_cartesian(xlim=c(100, 700)) +
geom_vline(xintercept=mean(bball$gdd[(bball$method=="ws" & bball$site=="arb")]), col=cols[[1]], linetype="dashed") +
geom_vline(xintercept=mean(bball$gdd[(bball$method=="ws" & bball$site=="hf")]), col=cols[[2]], linetype="dashed") +
xlab("Growing Degree Days (GDD)") + ylab("") +
scale_y_continuous(expand = c(0, 0)) +
#scale_x_continuous(breaks = seq(-20, 40, by=5)) +
theme(legend.position="none")
hobo <- ggplot(bball[(bball$method=="hobo"),], aes(x=gdd)) + geom_histogram(aes(fill=site), alpha=0.3, position="stack") +
theme_classic() +
scale_fill_manual(name="Site", values=cols, labels=c("Arboretum", "Harvard Forest")) + ggtitle("Hobo Logger") +
geom_vline(xintercept=mean(bball$gdd[(bball$method=="hobo" & bball$site=="arb")]), col=cols[[1]], linetype="dashed") +
geom_vline(xintercept=mean(bball$gdd[(bball$method=="hobo" & bball$site=="hf")]), col=cols[[2]], linetype="dashed") +
coord_cartesian(xlim=c(100, 700)) +
xlab("Growing Degree Days (GDD)") + ylab("") +
scale_y_continuous(expand = c(0, 0))
pdf("figures/gdd_methods_noisyws.pdf", width=8, height=4, onefile=FALSE)
egg::ggarrange(ws, hobo, ncol=2)
dev.off()
use.urban <- "urban"
bball$treatmenttype <- if(use.urban=="urban"){ifelse(bball$site=="arb", 1, 0)}else if(use.urban=="prov"){
as.numeric(bball$prov)}
datalist.gdd <- with(bball,
list(y = gdd,
urban = treatmenttype,
method = type,
sp = as.numeric(as.factor(species)),
N = nrow(bball),
n_sp = length(unique(bball$species))
)
)
brms::brm(gdd ~ treatmenttype + type + (treatmenttype + type|species), data=bball)
brms::get_prior(gdd ~ treatmenttype + type + (treatmenttype + type|species), data=bball)
# housekeeping
rm(list=ls())
options(stringsAsFactors = FALSE)
options(mc.cores = parallel::detectCores())
library(RColorBrewer)
library(viridis)
library(ggplot2)
library(gridExtra)
library(rstan)
setwd("~/Documents/git/microclimates/analyses/")
source("source/sims_hypoth_sourcedata.R")
source("source/sims_hypoth_interxn_sourcedata.R")
realgdd <- read.csv("output/cleanmicro_gdd_2019.csv")
simsdat <- bbfunc("hobo", "ws", 0, 15, 300, 20, 10, 3, 0)
xtext <- seq(1, 2, by=1)
cols <-viridis_pal(option="viridis")(3)
bball <- simsdat[[1]]
clim <- simsdat[[2]]
ws <- ggplot(clim[(clim$method=="ws"),], aes(x=tmean)) + geom_histogram(aes(fill=site), alpha=0.3) + theme_classic() +
scale_fill_manual(name="Site", values=cols, labels=c("Arboretum", "Harvard Forest")) + ggtitle("Weather Station") +
coord_cartesian(xlim=c(-20, 40)) +
geom_vline(xintercept=mean(clim$tmean[(clim$method=="ws" & clim$site=="arb")]), col=cols[[1]], linetype="dashed") +
geom_vline(xintercept=mean(clim$tmean[(clim$method=="ws" & clim$site=="hf")]), col=cols[[2]], linetype="dashed") +
xlab("Mean Temperature (°C)") + ylab("") +
scale_y_continuous(expand = c(0, 0)) +
scale_x_continuous(breaks = seq(-20, 40, by=5)) +
theme(legend.position="none")
hobo <- ggplot(clim[(clim$method=="hobo"),], aes(x=tmean)) + geom_histogram(aes(fill=site), alpha=0.3) + theme_classic() +
scale_fill_manual(name="Site", values=cols, labels=c("Arboretum", "Harvard Forest")) + ggtitle("Hobo Logger") +
geom_vline(xintercept=mean(clim$tmean[(clim$method=="hobo" & clim$site=="arb")]), col=cols[[1]], linetype="dashed") +
geom_vline(xintercept=mean(clim$tmean[(clim$method=="hobo" & clim$site=="hf")]), col=cols[[2]], linetype="dashed") +
coord_cartesian(xlim=c(-20, 40)) +
xlab("Mean Temperature (°C)") + ylab("") +
scale_y_continuous(expand = c(0, 0)) +
scale_x_continuous(breaks = seq(-20, 40, by=5))
pdf("figures/clim_methods_noisyws.pdf", width=8, height=4, onefile=FALSE)
egg::ggarrange(ws, hobo, ncol=2)
dev.off()
ws <- ggplot(bball[(bball$method=="ws"),], aes(x=gdd)) + geom_histogram(aes(fill=site), alpha=0.3, position="stack") +
theme_classic() +
scale_fill_manual(name="Site", values=cols, labels=c("Arboretum", "Harvard Forest")) + ggtitle("Weather Station") +
coord_cartesian(xlim=c(100, 700)) +
geom_vline(xintercept=mean(bball$gdd[(bball$method=="ws" & bball$site=="arb")]), col=cols[[1]], linetype="dashed") +
geom_vline(xintercept=mean(bball$gdd[(bball$method=="ws" & bball$site=="hf")]), col=cols[[2]], linetype="dashed") +
xlab("Growing Degree Days (GDD)") + ylab("") +
scale_y_continuous(expand = c(0, 0)) +
#scale_x_continuous(breaks = seq(-20, 40, by=5)) +
theme(legend.position="none")
hobo <- ggplot(bball[(bball$method=="hobo"),], aes(x=gdd)) + geom_histogram(aes(fill=site), alpha=0.3, position="stack") +
theme_classic() +
scale_fill_manual(name="Site", values=cols, labels=c("Arboretum", "Harvard Forest")) + ggtitle("Hobo Logger") +
geom_vline(xintercept=mean(bball$gdd[(bball$method=="hobo" & bball$site=="arb")]), col=cols[[1]], linetype="dashed") +
geom_vline(xintercept=mean(bball$gdd[(bball$method=="hobo" & bball$site=="hf")]), col=cols[[2]], linetype="dashed") +
coord_cartesian(xlim=c(100, 700)) +
xlab("Growing Degree Days (GDD)") + ylab("") +
scale_y_continuous(expand = c(0, 0))
pdf("figures/gdd_methods_noisyws.pdf", width=8, height=4, onefile=FALSE)
egg::ggarrange(ws, hobo, ncol=2)
dev.off()
use.urban <- "urban"
bball$treatmenttype <- if(use.urban=="urban"){ifelse(bball$site=="arb", 1, 0)}else if(use.urban=="prov"){
as.numeric(bball$prov)}
datalist.gdd <- with(bball,
list(y = gdd,
urban = treatmenttype,
method = type,
sp = as.numeric(as.factor(species)),
N = nrow(bball),
n_sp = length(unique(bball$species))
)
)
noisyws_fake = stan('stan/urbanmethod_normal_ncp_inter.stan', data = datalist.gdd,
iter = 2000, warmup=1500, chains=4, control=list(adapt_delta=0.99, max_treedepth=15))
