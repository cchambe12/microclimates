doy = do.call(c, days.btwprev))
climandpheno <- data.frame(id_year_type = rep.int(d$id_year_type, vapply(days.btwthis, length, 1L)),
doy = do.call(c, days.btwthis))
climandphenoprev <- separate(data = climandphenoprev, col = id_prevyear_type, into = c("id", "prevyear", "climatetype"), sep = "\\;")
climandpheno <- separate(data = climandpheno, col = id_year_type, into = c("id", "year", "climatetype"), sep = "\\;")
climandphenoprev$id_prevyear_type<-paste(climandphenoprev$id, climandphenoprev$prevyear, climandphenoprev$climatetype)
climandphenoprev$year <- as.numeric(climandphenoprev$prevyear) + 1
climandphenoprev$prevyear <- as.numeric(climandphenoprev$prevyear)
climandpheno$id_year_type<-paste(climandpheno$id, climandpheno$year, climandpheno$climatetype)
climandpheno$year <- as.numeric(climandpheno$year)
climandpheno$prevyear <- climandpheno$year-1
addhrs <- as.vector(unique(paste(climandpheno$id_year_type, climandpheno$doy)))
addhrs =
data.frame(
id_yr_day = sort(c(rep(addhrs, times=24))),
hour = sort(c(rep(seq(1:24))))
)
addhrsprev <- as.vector(unique(paste(climandphenoprev$id_prevyear_type, climandphenoprev$doy)))
addhrsprev =
data.frame(
id_yr_day = sort(c(rep(addhrsprev, times=24))),
hour = sort(c(rep(seq(1:24))))
)
climandpheno <- separate(data = addhrs, col = id_yr_day, into = c("id", "year", "climatetype", "doy"), sep = "\\ ")
climandpheno$id <- as.character(climandpheno$id)
climandpheno$year <- as.integer(climandpheno$year)
climandpheno$climatetype <- as.character(climandpheno$climatetype)
climandpheno$doy <- as.numeric(climandpheno$doy)
climandpheno$chill.endthis<-31
climandpheno$chill.startthis <- 1
climandphenoprev <- separate(data = addhrsprev, col = id_yr_day, into = c("id", "prevyear", "climatetype", "doy"), sep = "\\ ")
climandphenoprev$id <- as.character(climandphenoprev$id)
climandphenoprev$prevyear <- as.integer(climandphenoprev$prevyear)
climandphenoprev$climatetype <- as.character(climandphenoprev$climatetype)
climandphenoprev$doy <- as.numeric(climandphenoprev$doy)
climandphenoprev$chill.startprev <- ave(climandphenoprev$doy, climandphenoprev$id, climandphenoprev$prevyear, FUN=min)
climandphenoprev$chill.endprev <- ifelse(climandphenoprev$prevyear%in%leaps, 366, 365)
## Add Climate data back in
if(use.hobos==FALSE){
cc<-dplyr::select(cc, year, doy, tmean, hour, climatetype, precip)
}
if(use.hobos==TRUE){
cc<-dplyr::select(cc, year, doy, tmean, hour, climatetype)
}
cc$hour <- as.numeric(cc$hour)
climandpheno<-full_join(climandpheno, cc)
climandpheno<-climandpheno[!duplicated(climandpheno),]
climandpheno$id_year <- paste(climandpheno$id, climandpheno$year, sep="_")
climandphenoprev$year <- climandphenoprev$prevyear ### need to set up to get appropriate climate data
climandphenoprev <- full_join(climandphenoprev, cc)
climandphenoprev<-climandphenoprev[!duplicated(climandphenoprev),]
climandphenoprev$year <- as.numeric(climandphenoprev$year) + 1 ### need to put it back on the same page with other climandpheno
climandphenoprev$id_year <- paste(climandphenoprev$id, climandphenoprev$year, sep="_")
climandphenoall <- full_join(climandpheno, climandphenoprev)
bb_climandpheno<-full_join(climandphenoall, d)
bb_chilling_all <- subset(bb_climandpheno, select=c("id", "doy", "prevyear", "year", "tmean", "id_year"))
#bb_chilling_all <- na.omit(bb_chilling_all)
bb_chilling_all <- bb_chilling_all[!is.na(bb_chilling_all$tmean),]
bb_chilling_all <- bb_chilling_all[!is.na(bb_chilling_all$id),]
bb_chilling_all <- bb_chilling_all[(bb_chilling_all$year>2015),]
### Start with Treespotters and Harvard Forest
if(use.hobos==TRUE){
rms <- subset(bb_chilling_all, bb_chilling_all$year==2018)
rms <- unique(rms$id)
}
if(use.hobos==FALSE){
tt <- as.data.frame(table(bb_chilling_all$id, bb_chilling_all$year))
rms <- subset(tt, tt$Freq==0)
rms <- as.vector(unique(rms$Var1))
}
bb_chilling <- subset(bb_chilling_all, !bb_chilling_all$id %in% rms) ## need to do common garden inds after!!
period<-2019
nyears <- length(period)
ids<-bb_chilling[!duplicated(bb_chilling$id),]
ids <- ids[!duplicated(ids$id),]
ninds <- length(ids$id)
ids$idslist<-1:ninds
idlisttomerge <- subset(ids, select=c("id", "idslist"))
bb_chilling <- full_join(bb_chilling, idlisttomerge)
bb_chilling$doy <- as.numeric(bb_chilling$doy)
bb_chilling$base <- ifelse(!is.na(bb_chilling$prevyear),
ave(bb_chilling$doy, bb_chilling$id, bb_chilling$prevyear, FUN=min), NA)
bb_chilling$doy2 <- NA
bb_chilling$doy2 <- ifelse(!bb_chilling$prevyear%in%leaps & !is.na(bb_chilling$prevyear), bb_chilling$doy-273, bb_chilling$doy2)
bb_chilling$doy2 <- ifelse(!bb_chilling$prevyear%in%leaps & !is.na(bb_chilling$prevyear), bb_chilling$doy-273, bb_chilling$doy2)
bb_chilling$doy2 <- ifelse(is.na(bb_chilling$prevyear), bb_chilling$doy+92, bb_chilling$doy2)
bb_chilling$tmean <- round(bb_chilling$tmean, digits=3)
#bb_forcing <- bb_forcing[!is.na(bb_forcing$doy),]
extractchill<-function(tavg,period){
chillingyears<-array(NA,dim=c(nyears, 1, ninds))
row.names(chillingyears)<-period
colnames(chillingyears)<-c("utah")
yearlyresults<-array(NA,dim=c(length(period),1))
for(i in 1:ninds){ #i=2
print(i)
for(j in period){ #j=2019
print(paste(i,j))
days <- bb_chilling$doy2[bb_chilling$idslist==i & bb_chilling$year==j] #number of days of climate data
tavg <- bb_chilling$tmean[bb_chilling$idslist==i & bb_chilling$year==j]
hrly.temp =
data.frame(
Temp = tavg,
Year = rep(j, length(tavg)),
JDay = sort(days)
)
chillcalc.mn<-chilling(hrly.temp, hrly.temp$JDay[1], hrly.temp$JDay[nrow(hrly.temp[1])])
yearlyresults[which(period==j),1] <- chillcalc.mn$Utah_Model[which(chillcalc.mn$End_year==j)]
#yearlyresults[which(period==j),2] <- chillcalc.mn$Chill_portions[which(chillcalc.mn$End_year==j)]
}
chillingyears[,,i]<-yearlyresults
}
return(chillingyears)
}
chill_all <- extractchill(tavg, period)
allyears<-as.data.frame(chill_all)
allyears <- gather(allyears, idslist, utah)
allyears$year <- 2019
allyears$idslist <- as.numeric(substr(allyears$idslist, 6, 8))
bb_chilling <- subset(bb_chilling, select=c("id", "year", "idslist"))
bb_chilling <- bb_chilling[!duplicated(bb_chilling),]
allyears <- full_join(allyears, bb_chilling)
allyears <- dplyr::select(allyears, -idslist)
#############################
### Now for Common Garden ###
#############################
bb_chilling_cg <- subset(bb_chilling_all, bb_chilling_all$id %in% rms)
bb_chilling_cg <- subset(bb_chilling_cg, bb_chilling_cg$year%in%2018)
period_cg<-2018
nyears <- length(period_cg)
ids_cg<-bb_chilling_cg[!duplicated(bb_chilling_cg$id),]
ids_cg <- ids_cg[!duplicated(ids_cg$id),]
ninds_cg <- length(ids_cg$id)
ids_cg$idslist<-1:ninds_cg
idlisttomerge_cg <- subset(ids_cg, select=c("id", "idslist"))
bb_chilling_cg <- full_join(bb_chilling_cg, idlisttomerge_cg)
bb_chilling_cg$doy <- as.numeric(bb_chilling_cg$doy)
bb_chilling_cg$doy2 <- NA
bb_chilling_cg$doy2 <- ifelse(bb_chilling_cg$prevyear%in%leaps, bb_chilling_cg$doy-274, bb_chilling_cg$doy2)
bb_chilling_cg$doy2 <- ifelse(!bb_chilling_cg$prevyear%in%leaps & !is.na(bb_chilling_cg$prevyear), bb_chilling_cg$doy-273, bb_chilling_cg$doy2)
bb_chilling_cg$doy2 <- ifelse(is.na(bb_chilling_cg$prevyear), bb_chilling_cg$doy+92, bb_chilling_cg$doy2)
bb_chilling_cg$tmean <- round(bb_chilling_cg$tmean, digits=3)
bb_chilling_cg <- bb_chilling_cg[!is.na(bb_chilling_cg$doy),]
extractchill_cg<-function(ninds_cg,period_cg){
chillingyears_cg<-array(NA,dim=c(nyears, 1, ninds_cg))
row.names(chillingyears_cg)<-period_cg
colnames(chillingyears_cg)<-c("utah")
yearlyresults_cg<-array(NA,dim=c(length(period_cg),1))
for(i in 1:ninds_cg){ #i=1
print(i)
for(j in period_cg){ #j=2018
print(paste(i,j))
days <- bb_chilling_cg$doy2[bb_chilling_cg$idslist==i & bb_chilling_cg$year==j] #number of days of climate data
tavg_cg <- bb_chilling_cg$tmean[bb_chilling_cg$idslist==i & bb_chilling_cg$year==j]
hrly.temp =
data.frame(
Temp = tavg_cg,
Year = rep(j, length(days)),
JDay = sort(days)
)
chillcalc.mn<-chilling(hrly.temp, hrly.temp$JDay[1], hrly.temp$JDay[nrow(hrly.temp[1])])
yearlyresults_cg[which(period_cg==j),1] <- chillcalc.mn$Utah_Model[which(chillcalc.mn$End_year==j)]
#yearlyresults[which(period==j),2] <- chillcalc.mn$Chill_portions[which(chillcalc.mn$End_year==j)]
}
chillingyears_cg[,,i]<-yearlyresults_cg
}
return(chillingyears_cg)
}
chill_all_cg <- extractchill_cg(ninds_cg, period_cg)
oneyear<-as.data.frame(chill_all_cg)
oneyear <- gather(oneyear, idslist, utah)
oneyear$year <- 2018
oneyear$idslist <- as.numeric(substr(oneyear$idslist, 6, 8))
bb_chilling_cg <- subset(bb_chilling_cg, select=c("id", "year", "idslist"))
bb_chilling_cg <- bb_chilling_cg[!duplicated(bb_chilling_cg),]
oneyear <- full_join(oneyear, bb_chilling_cg)
oneyear <- dplyr::select(oneyear, -idslist)
chillbb <- full_join(allyears, oneyear)
gdd.stan <- left_join(gdd.stan, chillbb)
} else {
print("Error: chillbb not a data.frame")
}
View(gdd.stan)
bb_chilling <- subset(bb_chilling_all, !bb_chilling_all$id %in% rms) ## need to do common garden inds after!!
period<-2019
nyears <- length(period)
ids<-bb_chilling[!duplicated(bb_chilling$id),]
ids <- ids[!duplicated(ids$id),]
ninds <- length(ids$id)
ids$idslist<-1:ninds
idlisttomerge <- subset(ids, select=c("id", "idslist"))
bb_chilling <- full_join(bb_chilling, idlisttomerge)
bb_chilling$doy <- as.numeric(bb_chilling$doy)
bb_chilling$base <- ifelse(!is.na(bb_chilling$prevyear),
ave(bb_chilling$doy, bb_chilling$id, bb_chilling$prevyear, FUN=min), NA)
bb_chilling$doy2 <- NA
bb_chilling$doy2 <- ifelse(!bb_chilling$prevyear%in%leaps & !is.na(bb_chilling$prevyear), bb_chilling$doy-273, bb_chilling$doy2)
bb_chilling$doy2 <- ifelse(!bb_chilling$prevyear%in%leaps & !is.na(bb_chilling$prevyear), bb_chilling$doy-273, bb_chilling$doy2)
bb_chilling$doy2 <- ifelse(is.na(bb_chilling$prevyear), bb_chilling$doy+92, bb_chilling$doy2)
bb_chilling$tmean <- round(bb_chilling$tmean, digits=3)
View(bb_chilling)
chillingyears<-array(NA,dim=c(nyears, 1, ninds))
row.names(chillingyears)<-period
colnames(chillingyears)<-c("utah")
yearlyresults<-array(NA,dim=c(length(period),1))
i=2
print(i)
j=2019
print(paste(i,j))
days <- bb_chilling$doy2[bb_chilling$idslist==i & bb_chilling$year==j] #number of days of climate data
days
tavg <- bb_chilling$tmean[bb_chilling$idslist==i & bb_chilling$year==j]
tavg
hrly.temp =
data.frame(
Temp = tavg,
Year = rep(j, length(tavg)),
JDay = sort(days)
)
hrly.temp
chillcalc.mn<-chilling(hrly.temp, hrly.temp$JDay[1], hrly.temp$JDay[nrow(hrly.temp[1])])
yearlyresults[which(period==j),1] <- chillcalc.mn$Utah_Model[which(chillcalc.mn$End_year==j)]
chillingyears[,,i]<-yearlyresults
chillingyears
chill_all <- extractchill(tavg, period)
allyears<-as.data.frame(chill_all)
allyears
allyears <- gather(allyears, idslist, utah)
allyears
allyears$year <- 2019
allyears$idslist <- as.numeric(substr(allyears$idslist, 6, 8))
bb_chilling <- subset(bb_chilling, select=c("id", "year", "idslist"))
bb_chilling <- bb_chilling[!duplicated(bb_chilling),]
allyears <- full_join(allyears, bb_chilling)
allyears
allyears <- dplyr::select(allyears, -idslist)
### 24 January 2019 - Cat
## Fresh start to adding in climate data
# Trying to calculate chilling and forcing for budburst and leafout
### Weather data for the Arboretum downloaded from... http://labs.arboretum.harvard.edu/weather/
##  not using hobo loggers
### Weather data for Harvard Forests downloaded from... http://harvardforest.fas.harvard.edu:8080/exist/apps/datasets/showData.html?id=hf001
## and select 'hf001-10' - not using hobo loggers. Takes a while to download.
# ## housekeeping
rm(list=ls())
options(stringsAsFactors = FALSE)
## Load Libraries
library(dplyr)
library(tidyr)
library(geosphere)
library(anytime)
library(weathermetrics)
library(measurements)
library(lubridate)
library(chillR)
# Set Working Directory
setwd("~/Documents/git/microclimates/analyses")
d <- read.csv("output/clean_budburstandleafout.csv", header=TRUE)
## Flags for question
use.hobos <- TRUE ## make false if want to use main station climate data rather than the hobo loggers
### For #1, must choose whether you want hobo logger data or main climate towers
if(use.hobos==FALSE){
# 1a. Let's add in climate data first for forcing.
source("calculating/clean_addinclimate.R") ## takes a while to load all the data, brings in climate data
#write.csv(cc, file="output/clean_addinclimate.csv", row.names=FALSE)
}
if(use.hobos==TRUE){
# 1b. Let's add in climate data from each hobo logger.
source("calculating/clean_addinclimate_loggers.R") ## takes a while to load all the data, brings in climate data
#write.csv(cc, file="output/clean_addinclimate.csv", row.names=FALSE)
}
# 2. Let's add in Forcing data first. We will use February 15 as the start
# of calculating GDD. Easy to fix if necessary in the gdd.start column
if(use.hobos==TRUE){
## Need to reset the working directory if use.hobos==TRUE.
setwd("~/Documents/git/microclimates/analyses")
}
source("calculating/calc_forceBB.R") ### This part can take a while depending on how many years of data you have and how many loggers
# 2. Let's add in Forcing data for leafoutnow. We will again use February 15 as the start
# of calculating GDD. Easy to fix if necessary in the gdd.start column
source("calculating/calc_forceLO.R") ### This part can take a while depending on how many years of data you have and how many loggers
# 3. Now let's add in forcing from budburst to leafout!! And also add in false spring information
source("calculating/calc_forceDVR.R") ### This part can take a while depending on how many years of data you have and how many loggers
# 4. Let's add in Chilling data for budburst now. We will use February 15 as the end
# of calculating chill and start with last observation from prev season. Easy to fix if necessary in the chill.startthis column
source("calculating/calc_chillports.R") ### This part can take a while depending on how many years of data you have and how many loggers
if(use.hobos==FALSE){
# 4. Let's add in tmean for the growing season for each individual - does climate play a roll on growing season length?
source("calculating/calc_gstmean.R")
}
if(use.hobos==FALSE){
# 5. Finally, let's add in precip for the growing season for each individual - does precip play a roll on growing season length?
source("calculating/calc_precip.R")
}
if(use.hobos==FALSE){
## If using 1a)...
write.csv(gdd.stan, file="output/clean_gdd_chill_bbanddvr.csv", row.names = FALSE)
}
if(use.hobos==TRUE){
## If using 1b)...
write.csv(gdd.stan, file="output/clean_gdd_chill_bbanddvr_hobo.csv", row.names = FALSE)
}
View(gdd.stan)
View(bb_chilling)
### 24 January 2019 - Cat
## Fresh start to adding in climate data
# Trying to calculate chilling and forcing for budburst and leafout
### Weather data for the Arboretum downloaded from... http://labs.arboretum.harvard.edu/weather/
##  not using hobo loggers
### Weather data for Harvard Forests downloaded from... http://harvardforest.fas.harvard.edu:8080/exist/apps/datasets/showData.html?id=hf001
## and select 'hf001-10' - not using hobo loggers. Takes a while to download.
# ## housekeeping
rm(list=ls())
options(stringsAsFactors = FALSE)
## Load Libraries
library(dplyr)
library(tidyr)
library(geosphere)
library(anytime)
library(weathermetrics)
library(measurements)
library(lubridate)
library(chillR)
# Set Working Directory
setwd("~/Documents/git/microclimates/analyses")
d <- read.csv("output/clean_budburstandleafout.csv", header=TRUE)
## Flags for question
use.hobos <- FALSE ## make false if want to use main station climate data rather than the hobo loggers
### For #1, must choose whether you want hobo logger data or main climate towers
if(use.hobos==FALSE){
# 1a. Let's add in climate data first for forcing.
source("calculating/clean_addinclimate.R") ## takes a while to load all the data, brings in climate data
#write.csv(cc, file="output/clean_addinclimate.csv", row.names=FALSE)
}
if(use.hobos==TRUE){
# 1b. Let's add in climate data from each hobo logger.
source("calculating/clean_addinclimate_loggers.R") ## takes a while to load all the data, brings in climate data
#write.csv(cc, file="output/clean_addinclimate.csv", row.names=FALSE)
}
# 2. Let's add in Forcing data first. We will use February 15 as the start
# of calculating GDD. Easy to fix if necessary in the gdd.start column
if(use.hobos==TRUE){
## Need to reset the working directory if use.hobos==TRUE.
setwd("~/Documents/git/microclimates/analyses")
}
source("calculating/calc_forceBB.R") ### This part can take a while depending on how many years of data you have and how many loggers
# 2. Let's add in Forcing data for leafoutnow. We will again use February 15 as the start
# of calculating GDD. Easy to fix if necessary in the gdd.start column
source("calculating/calc_forceLO.R") ### This part can take a while depending on how many years of data you have and how many loggers
# 3. Now let's add in forcing from budburst to leafout!! And also add in false spring information
source("calculating/calc_forceDVR.R") ### This part can take a while depending on how many years of data you have and how many loggers
# 4. Let's add in Chilling data for budburst now. We will use February 15 as the end
# of calculating chill and start with last observation from prev season. Easy to fix if necessary in the chill.startthis column
source("calculating/calc_chillports.R") ### This part can take a while depending on how many years of data you have and how many loggers
if(use.hobos==FALSE){
# 4. Let's add in tmean for the growing season for each individual - does climate play a roll on growing season length?
source("calculating/calc_gstmean.R")
}
if(use.hobos==FALSE){
# 5. Finally, let's add in precip for the growing season for each individual - does precip play a roll on growing season length?
source("calculating/calc_precip.R")
}
if(use.hobos==FALSE){
## If using 1a)...
write.csv(gdd.stan, file="output/clean_gdd_chill_bbanddvr.csv", row.names = FALSE)
}
if(use.hobos==TRUE){
## If using 1b)...
write.csv(gdd.stan, file="output/clean_gdd_chill_bbanddvr_hobo.csv", row.names = FALSE)
}
rm(list=ls())
options(stringsAsFactors = FALSE)
#### Questions to address:
## Compare GDDs between hobo loggers and weather station data
# GDDlo ~ urban + (urban|species) - do once with weather station data and once with hobo logger data
## Let's start with Question 1 first...
#library(rethinking)
library(RColorBrewer)
library(lme4)
## Let's load some real data to check out.
setwd("~/Documents/git/microclimates/analyses/")
ws <- read.csv("output/clean_gdd_chill_bbanddvr.csv")
hobo <- read.csv("output/clean_gdd_chill_bbanddvr_hobo.csv")
View(ws)
# We will focus on Harvard Forest to start
hf.ws <- ws[(ws$type=="Treespotters"),]
hf.hobo <- hobo[(hobo$type=="Treespotters"),]
#################################################################################################
#################################    Now for CHILLING!   ########################################
#################################################################################################
mean(hf.ws$utah, na.rm=TRUE) ## 877.41
mean(hf.hobo$utah, na.rm=TRUE) ## 541.2
nsp = 20 # number of species
ntot = 200 # numbers of obs per species.
sample_a <- list(site.env = rbinom(1000, 1, 0.5))
model.parameters <- list(intercept = 850,
urban.coef = -150)
#  2) Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = nsp * ntot, replace = TRUE)})
mm <- model.matrix(~env.samples)
#mm <- mm[,-2]
#  4) We need to make a random intercept model for each species
parameters.temp <- matrix(unlist(model.parameters), ncol = length(model.parameters), nrow = nsp * ntot, byrow = TRUE)
# Which parameters are random?
random.regex <- grep(pattern = paste(c("intercept", "urban.coef"), collapse = "|"), x = names(model.parameters))
# Generate random parameters (by species)
parameters.temp[, 1] <- sapply(1:nsp, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters[[random.regex[1]]], sd = 100), ntot)})
parameters.temp[, 2] <- sapply(1:nsp, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters[[random.regex[2]]], sd = 50), ntot)})
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 30)})
fakedata_ws_urb_chill <- cbind(data.frame(species = as.vector(sapply(1:nsp, FUN = function(x) rep(x, ntot))),
utah = response, urban = env.samples[,1]))
write.csv(fakedata_ws_urb_chill, file="output/fakedata_ws_urb_chill.csv", row.names = FALSE)
#  7) Let's do a quick lmer model to test the fake data
modtest <- lmer(utah ~ urban + (urban|species), data=fakedata_ws_urb_chill) ## Quick look looks good!
modtest
nsp = 20 # number of species
ntot = 200 # numbers of obs per species.
sample_a <- list(site.env = rbinom(1000, 1, 0.5))
model.parameters <- list(intercept = 1050,
urban.coef = -200)
#  2) Now, we will make varying intercepts
env.samples <- sapply(sample_a, FUN = function(x){
sample(x, size = nsp * ntot, replace = TRUE)})
mm <- model.matrix(~env.samples)
#mm <- mm[,-2]
#  4) We need to make a random intercept model for each species
parameters.temp <- matrix(unlist(model.parameters), ncol = length(model.parameters), nrow = nsp * ntot, byrow = TRUE)
# Which parameters are random?
random.regex <- grep(pattern = paste(c("intercept", "urban.coef"), collapse = "|"), x = names(model.parameters))
# Generate random parameters (by species)
parameters.temp[, 1] <- sapply(1:nsp, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters[[random.regex[1]]], sd = 150), ntot)})
parameters.temp[, 2] <- sapply(1:nsp, FUN = function(x){
rep(rnorm(n = 1, mean = model.parameters[[random.regex[2]]], sd = 75), ntot)})
# Calculate response
response <- sapply(1:nrow(env.samples), FUN = function(x){
rnorm(n = 1, mean = mm[x, ] %*% parameters.temp[x, ], sd = 30)})
fakedata_hl_urb_chill <- cbind(data.frame(species = as.vector(sapply(1:nsp, FUN = function(x) rep(x, ntot))),
utah = response, urban = env.samples[,1]))
#  7) Let's do a quick lmer model to test the fake data
modtesthl <- lmer(utah ~ urban + (urban|species), data=fakedata_hl_urb_chill) ## Quick look looks good!
modtesthl
write.csv(fakedata_hl_urb_chill, file="output/fakedata_hl_urb_chill.csv", row.names = FALSE)
###############################################################################################################
################################ URBAN MODELS with FAKE chill data ############################################
###############################################################################################################
ws_urb_chill <- read.csv("output/fakedata_ws_urb_chill.csv")
hobo_urb_chill <- read.csv("output/fakedata_hl_urb_chill.csv")
datalist.wsurb.chill <- with(ws_urb_chill,
list(y = utah,
tx = urban,
sp = as.numeric(as.factor(species)),
N = nrow(ws_urb),
n_sp = length(unique(ws_urb$species))
)
)
rm(list=ls())
options(stringsAsFactors = FALSE)
#### Questions to address:
## Compare GDDs between hobo loggers and weather station data
# 1) GDDlo ~ 1 + (1|species) - do once for HF weather station, once for hobo logger and repeat for Arboretum
# Compare urban effect using weather station data and then hobo logger data
# 2) GDDlo ~ urban + (urban|species) - do once with weather station data and once with hobo logger data
## Let's start with Question 1 first...
library(bayesplot) ## for plotting
library(egg) ## for plotting
library(shinystan)
library(rstanarm)
library(rstan)
library(brms)
## Let's load some real data to check out.
setwd("~/Documents/git/microclimates/analyses/")
source("source/stan_utility.R")
###############################################################################################################
################################ URBAN MODELS with FAKE chill data ############################################
###############################################################################################################
ws_urb_chill <- read.csv("output/fakedata_ws_urb_chill.csv")
hobo_urb_chill <- read.csv("output/fakedata_hl_urb_chill.csv")
datalist.wsurb.chill <- with(ws_urb_chill,
list(y = utah,
tx = urban,
sp = as.numeric(as.factor(species)),
N = nrow(ws_urb_chill),
n_sp = length(unique(ws_urb_chill$species))
)
)
ws_urb_fake_chill = stan('stan/urbanchill_stan_normal_weather.stan', data = datalist.wsurb.chill,
iter = 5000, warmup=2000, control=list(max_treedepth = 15,adapt_delta = 0.99)) ###
ws_urb_fake_chill = stan('stan/urbanchill_stan_normal_weather.stan', data = datalist.wsurb.chill,
iter = 5000, warmup=2000, control=list(max_treedepth = 15,adapt_delta = 0.99)) ###
check_all_diagnostics(ws_urb_fake_chill)
ws_urb_fake_chill.sum <- summary(ws_urb_fake_chill)$summary
ws_urb_fake_chill.sum[grep("mu_", rownames(ws_urb_fake_chill.sum)),]
ws_urb_fake_chill.sum[grep("sigma_", rownames(ws_urb_fake_chill.sum)),]
datalist.hlurb.chill <- with(hl_urb_chill,
list(y = utah,
tx = urban,
sp = as.numeric(as.factor(species)),
N = nrow(hl_urb_chill),
n_sp = length(unique(hl_urb_chill$species))
)
)
hl_urb_fake_chill = stan('stan/urbanchill_stan_normal_hobo.stan', data = datalist.hlurb.chill,
iter = 5000, warmup=2000, control=list(max_treedepth = 15,adapt_delta = 0.99)) ###
hl_urb_fake_chill = stan('stan/urbanchill_stan_normal_hobo.stan', data = datalist.hlurb.chill,
iter = 5000, warmup=2000) ###
hl_urb_fake_chill = stan('stan/urbanchill_stan_normal_hobo.stan', data = datalist.hlurb.chill,
iter = 5000, warmup=2000) ###
